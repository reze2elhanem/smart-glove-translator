<!--
  ===================    !!READ THIS NOTICE!!   ====================
  DO NOT edit this file manually. Your changes WILL BE OVERWRITTEN!
  This journal is auto generated and updated by Hack Club Blueprint.
  To edit this file, please edit your journal entries on Blueprint.
  ==================================================================
-->

This is my journal of the design and building process of **Smart  Translator Gloves**.  
You can view this journal in more detail on **Hack Club Blueprint** [here](https://blueprint.hackclub.com/projects/693).


## 10/18/2025 - Brainstormed, designed  prototype, and refined project idea  

Today, I spent![ Image 18 أكتوبر 2025، 02_29_12 م.png](https://blueprint.hackclub.com/user-attachments/blobs/proxy/eyJfcmFpbHMiOnsiZGF0YSI6MzA4NSwicHVyIjoiYmxvYl9pZCJ9fQ==--9860dae6e8a1b2700b77f9d282a461c6378be015/ChatGPT%20Image%2018%20%D8%A3%D9%83%D8%AA%D9%88%D8%A8%D8%B1%202025%D8%8C%2002_29_12%20%D9%85.png)
 !
 hours brainstorming and shaping my project idea, Sign2Speech. At first, I struggled to figure out how to actually turn sign language into speech  it felt like a huge technical challenge. I started researching each sensor one by one to understand how they work and how they could fit into my idea.

At first, I tried using AI to design a 3D prototype so I could visualize my concept better, but honestly, the results weren’t exactly what I had in mind  it helped me imagine things more clearly though. After more searching and testing different possibilities, I realized that using Flex Sensors and the MPU6050 together would give the most accurate and smooth results.

It was a mix of frustration and excitement, but by the end of the day, I finally managed to put the concept together in a clear way and start writing my project description. I feel like the idea is becoming more real step by step.  

## 10/19/2025 - Learning the basics of sign language   

Today was such an exciting day! I started learning the basics of Egyptian Arabic sign language  I focused on about ten simple words, trying to understand each hand movement and the angle behind it. It honestly felt magical to see how much meaning can be expressed just by a few gestures.

Then, I moved on to the coding part. I managed to write the initial code for two words  “hi” and “how are you”  in arabic (egyptian accent) ![WhatsApp Image 2025-10-19 at 5.44.02 PM.jpeg](https://blueprint.hackclub.com/user-attachments/blobs/proxy/eyJfcmFpbHMiOnsiZGF0YSI6MzM3NywicHVyIjoiYmxvYl9pZCJ9fQ==--271b1f7f26dc60cc6e2e695b1d7816966a4e4c80/WhatsApp%20Image%202025-10-19%20at%205.44.02%20PM.jpeg)
since they’re usually the start of any conversation. Seeing the LCD display respond the right way gave me such a good feeling it was like the project finally started breathing a little

LaterI spent some time brainstorming future features I’d love to add
One idea I really like is adding a small mic that could actually say the translated words out loud. Another idea is to make the glove smart enough to “learn” new gestures automatically without needing to code each one manually
It’s just day two, but I can already imagine how far this could go   

